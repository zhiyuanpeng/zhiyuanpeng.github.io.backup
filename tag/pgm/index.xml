<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>pgm | Academic</title>
    <link>https://zhiyuanpeng.github.io/tag/pgm/</link>
      <atom:link href="https://zhiyuanpeng.github.io/tag/pgm/index.xml" rel="self" type="application/rss+xml" />
    <description>pgm</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Thu, 13 May 2021 13:59:30 -0700</lastBuildDate>
    <image>
      <url>https://zhiyuanpeng.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_2.png</url>
      <title>pgm</title>
      <link>https://zhiyuanpeng.github.io/tag/pgm/</link>
    </image>
    
    <item>
      <title>Representation of Undirected Graph Models</title>
      <link>https://zhiyuanpeng.github.io/post/representation-of-undirected-graph-models/</link>
      <pubDate>Thu, 13 May 2021 13:59:30 -0700</pubDate>
      <guid>https://zhiyuanpeng.github.io/post/representation-of-undirected-graph-models/</guid>
      <description>&lt;style&gt;
body {
text-align: justify}
&lt;/style&gt;
&lt;p&gt;Koller/Friedman&amp;rsquo;s book &lt;strong&gt;Probabilistic Graphical Models: Principles and Techniques&lt;/strong&gt; &lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; (PGM) is the most exhaustive book I have found for PGM, however, there are many definitions, theorems and mathematics within it which make the book hard to understand. When I read the chapter 4 of this book, I meet two questions: one is why the potential functions are defined on cliques, another is why the d-seperation works. For the first question, I didn&amp;rsquo;t find a good answer from Koller/Friedman&amp;rsquo;s book, but, I found Micheal I. Jordan&amp;rsquo;s unfinished book &lt;strong&gt;An Introduction to Probabilistic Graphical Model&lt;/strong&gt; &lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; gives some intitutions of why we do it like that. For the second one, Koller/Friedman&amp;rsquo;s book definies the d-seperation in definition 4.9 and proves the soundness in theorem 4.1. When I first read the theorem 4.1, I didn&amp;rsquo;t understand why the proof of theorem 4.1 could proof the soundness of definition 4.9. I got it after I read that part several times and I will share what I learnt in this post.&lt;/p&gt;
&lt;h1 id=&#34;prerequisites&#34;&gt;Prerequisites&lt;/h1&gt;
&lt;h2 id=&#34;some-important-concepts&#34;&gt;Some Important Concepts&lt;/h2&gt;
&lt;p&gt;If you feel Koller/Friedman&amp;rsquo;s PGM difficult to digest, you may not familar with some concepts, so, we first clarify some concepts that I was not know when I first read this book:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;completeness: An axiom system is complete if all valid statements can be derived.&lt;/li&gt;
&lt;li&gt;soundness: An axiom system is sound if only valid statements can be derived.&lt;/li&gt;
&lt;li&gt;satisfiability: We say that a truth assignment $I$ satisfies a formula $\varphi$ if $\varphi[I]=\mathrm{T} ;$ we write this as $I \models \varphi$. A formula $\varphi$ is satisfiable if there exists a truth assignment $I$ such that $I \models \varphi$; otherwise $\varphi$ is unsatisfiable.&lt;/li&gt;
&lt;li&gt;validity: A formula $\varphi$ is valid (or a tautology) if for all a truth assignments $I, I \models \varphi$.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;proof-of-exercise-25&#34;&gt;Proof of Exercise 2.5&lt;/h2&gt;
&lt;p&gt;In Koller/Friedman&amp;rsquo;s PGM book, the conclusion of exercise 2.5 is directly used to proof other theorems, but no proof is given. I googled this answer on the Internet, luckly, I found it and the proof is right in my view.&lt;/p&gt;
&lt;p&gt;Exercise 2.5:&lt;/p&gt;
&lt;p&gt;Let $X, Y, Z$ be three disjoint subsets of random variables. We say $X$ and $Y$ are conditionally independent given $Z$ if and only if&lt;/p&gt;
&lt;p&gt;$$
\mathbb P_{X, Y \mid Z}(x, y \mid z)=\mathbb P_{X \mid Z}(x \mid z) \mathbb P_{Y \mid Z}(y \mid z)
$$&lt;/p&gt;
&lt;p&gt;Show that $X$ and $Y$ are conditionally independent given $Z$ if and only if the joint distribution for the three subsets of random variables factors in the following form:&lt;/p&gt;
&lt;p&gt;$$
\mathbb P_{X, Y, Z}(x, y, \mid z)=h(x, z) g(y, z))
$$&lt;/p&gt;
&lt;p&gt;Solution:&lt;/p&gt;
&lt;p&gt;Fist, we proof if $X$ and $Y$ are conditionally independent given $Z$, we can get $\mathbb P_{X, Y, Z}(x, y, \mid z)=h(x, z) g(y, z))$ as follows:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\mathbb P_{X, Y, Z}(x, y, z) &amp;amp;= \mathbb P_{Z}(z) \mathbb P_{X, Y | Z}(x, y | z) \\&lt;br&gt;
&amp;amp;=\mathbb P_{Z}(z) \mathbb P_{X | Z}(x | z) \mathbb P_{Y | Z}(y | z) \\&lt;br&gt;
&amp;amp;=h(x, z) g(y, z)
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;where we choose $h(x, z)=\mathbb P_{X \mid Z}(x \mid z)$ and $g(y, z) = \mathbb P_{Y \mid Z}(y \mid z) \mathbb P_{Z}(z)$&lt;/p&gt;
&lt;p&gt;Then, we proof that if we can write $\mathbb P_{X, Y, Z}(x, y, z)$ in this form: $h(x, z) g(y, z))$, we can get $\mathbb P_{X, Y \mid Z}(x, y \mid z)=\mathbb P_{X \mid Z}(x \mid z) \mathbb P_{Y \mid Z}(y \mid z)$:&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\mathbb P_{X, Y \mid Z}(z, y \mid z) &amp;amp;=\frac{\mathbb P_{X, Y, Z}(x, y, z)}{\sum_{x, y} \mathbb P_{X, Y, Z}(x, y, z)} \\&lt;br&gt;
&amp;amp;=\frac{h(x, z) g(y, z)}{\sum_{x, y} h(x, z) g(y, z)} \\&lt;br&gt;
&amp;amp;=\frac{h(x, z)}{h_{1}(z)} \frac{g(y, z)}{ g_{1}(z)} \\&lt;br&gt;
&amp;amp;=\mathbb P_{X \mid Z}(x \mid z) \mathbb P_{Y \mid Z}(y \mid z)
\end{aligned}
$$&lt;/p&gt;
&lt;h2 id=&#34;definition-44&#34;&gt;Definition 4.4&lt;/h2&gt;
&lt;p&gt;We say that a distribution factorizes over a Markov network $\mathcal{H}$ if each $\boldsymbol{D}_{k}(k=1, \ldots, K)$ is a complete subgraph of $\mathcal{H}$.&lt;/p&gt;
&lt;h2 id=&#34;gibbs-distribution&#34;&gt;Gibbs Distribution&lt;/h2&gt;
&lt;p&gt;A distribution $P_{\Phi}$ is a Gibbs distribution parameterized by a set of factors $\Phi=\left\{\phi_{1}\left(\boldsymbol{D}_{1}\right), \ldots, \phi_{K}\left(\boldsymbol{D}_{K}\right)\right\}$ if it is defined as follows:
$$
P_{\Phi}\left(X_{1}, \ldots, X_{n}\right)=\frac{1}{Z} \tilde{P}_{\Phi}\left(X_{1}, \ldots, X_{n}\right)
$$
where
$$
\tilde{P}_{\Phi}\left(X_{1}, \ldots, X_{n}\right)=\phi_{1}\left(\boldsymbol{D}_{1}\right) \times \phi_{2}\left(\boldsymbol{D}_{2}\right) \times \cdots \times \phi_{m}\left(\boldsymbol{D}_{m}\right)
$$
is an unnormalized measure and
$$
Z=\sum_{X_{.}} \tilde{P}_{\Phi}\left(X_{1}, \ldots, X_{n}\right)
$$&lt;/p&gt;
&lt;p&gt;If we say a Gibbs distribution which factorizes over a Markov network, each factor in this Gibbs distribution is over a clique or complete subgraph.&lt;/p&gt;
&lt;h2 id=&#34;definition-48&#34;&gt;Definition 4.8&lt;/h2&gt;
&lt;p&gt;Let $\mathcal{H}$ be a Markov network structure, and let $X_{1}-\ldots-X_{k}$ be a path in $\mathcal{H}$. Let $\boldsymbol{Z} \subseteq \mathcal{X}$ be a set of observed variables. The path $X_{1}-\ldots-X_{k}$ is active given $\boldsymbol{Z}$ if none of the $X_{i}$ &amp;rsquo;s, $i=1, \ldots, k$, is in $Z$.&lt;/p&gt;
&lt;h1 id=&#34;why-udg-is-defined-on-cliques&#34;&gt;Why UDG is defined on cliques?&lt;/h1&gt;
&lt;p&gt;$A&amp;ndash;B&amp;ndash;C$&lt;/p&gt;
&lt;p&gt;From the graph above, we can get $A \bot C \mid B$, so, we don&amp;rsquo;t want $A$ and $C$ in the same potential function because they are independent if $B$ is given. We notice that there is no edge connecting $A$ and $C$ directly, so, we may have an intitution that nodes are not connected to each other should not be included in the the same potential function. This is the intuition why UDG is defined on cliques. We use potentional functions to measture the relationship among nodes within the same clique, not the conditional probability or marginal probability. For instance, we can use the chain rule to write the distribution $P$ factorizing over the graph above like this: $P = P(A)P(B \mid A)P(C \mid B)$. If you push $P$ into two factors, you may select one of the two forms: $P = P(A,B)P(C \mid B)$ and $P = P(B, C)P(A \mid B)$. Don&amp;rsquo;t forget that our distribution defined on UDG can be factorized like this $P = \Phi(A, B)\Phi(B,C)$. Then, we can get the conclusion that not all the potential functions are marginal probabilities or conditional probabilities. In one word, potential functions are not probabilities. How can we make sure the distribution consisting of potential equal to 1 if we summarize all the variables within the distribution? We need a nomarlizer $\mathcal{Z}$. From the definition 4.4 and the definition of Gibbs distribution, we know that we can use a Gibbs distribution which factorizes over the Markov network to represent the distribution expressed by the Markov network.&lt;/p&gt;
&lt;p&gt;Koller/Firedman says in the PGM book &amp;ldquo;if we define the UDG on cliques, we will not violate the independence assumptions induced by the network structure&amp;rdquo;. In my view, the authors here want to say that if we use the Gibbs distributiuon which factorizes over Markov network to represent the distribution expressed by the network, we can proof the theorem 4.1 which also proofs the soundness of the d-seperation which is the independence assumptions induced by the network.&lt;/p&gt;
&lt;h1 id=&#34;why-d-seperation-works&#34;&gt;Why D-seperation works?&lt;/h1&gt;
&lt;p&gt;Definition 4.9 defines the d-seperation of UDG. The proof of definition 4.9 will answer this question.&lt;/p&gt;
&lt;p&gt;Definition 4.9:&lt;/p&gt;
&lt;p&gt;We say that a set of nodes $\boldsymbol{Z}$ separates $\boldsymbol{X}$ and $\boldsymbol{Y}$ in $\mathcal{H}$, denoted $\operatorname{sep}_{\mathcal{H}}(\boldsymbol{X} ; \boldsymbol{Y} \mid \boldsymbol{Z})$, if there is no active path between any node $X \in X$ and $Y \in \boldsymbol{Y}$ given $\boldsymbol{Z}$. We define the global independencies associated with $\mathcal{H}$ to be:&lt;/p&gt;
&lt;p&gt;$$
\mathcal{I}(\mathcal{H})=\left\{(\boldsymbol{X} \perp \boldsymbol{Y} \mid \boldsymbol{Z}): \operatorname{sep}_{\mathcal{H}}(\boldsymbol{X} ; \boldsymbol{Y} \mid \boldsymbol{Z})\right\}
$$&lt;/p&gt;
&lt;p&gt;We proof the soundness and completeness of definition 4.9. For definition 4.9, the soundness and coompleteness are defined as follows:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If we find that two nodes $X$ and $Y$ are d-separated given some $Z$, then we are guaranteed that they are, in fact, conditionally independent given $Z$.&lt;/li&gt;
&lt;li&gt;D-separation detects all possible independencies. More precisely, if we have that two variables $X$ and $Y$ are independent given $Z$, then they are d-separated.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Before we proof the soundness, we know that there is a distribution $\mathcal{P}$ over $\mathcal{X}$ factorizes over $\mathcal{H}$ can be expressed by the Markov network $\mathcal{H}$. Let $\boldsymbol{X}, \boldsymbol{Y}, \boldsymbol{Z}$ be any three disjoint subsets in $\mathcal{X}$ such that $\boldsymbol{Z}$ separates $\boldsymbol{X}$ and $\boldsymbol{Y}$ in $\mathcal{H}$. We want to show that $P \models(\boldsymbol{X} \perp \boldsymbol{Y} \mid \boldsymbol{Z})$.&lt;/p&gt;
&lt;p&gt;We start by considering the case where $\boldsymbol{X} \cup \boldsymbol{Y} \cup \boldsymbol{Z}=\mathcal{X} .$ As $\boldsymbol{Z}$ separates $\boldsymbol{X}$ from $\boldsymbol{Y}$, there are no direct edges between $\boldsymbol{X}$ and $\boldsymbol{Y}$. Hence, any clique in $\mathcal{H}$ is fully contained either in $\boldsymbol{X} \cup \boldsymbol{Z}$ or in $\boldsymbol{Y} \cup \boldsymbol{Z}$. Let $\mathcal{I}_{\boldsymbol{X}}$ be the indexes of the set of cliques that are contained in $\boldsymbol{X} \cup \boldsymbol{Z}$, and let $\mathcal{I}_{Y}$ be the indexes of the remaining cliques. We know that&lt;/p&gt;
&lt;p&gt;$$
P\left(X_{1}, \ldots, X_{n}\right)=\frac{1}{Z} \prod_{i \in \mathcal{I}_{\boldsymbol{X}}} \phi_{i}\left(\boldsymbol{D}_{i}\right) \cdot \prod_{i \in \mathcal{I}_{\boldsymbol{Y}}} \phi_{i}\left(\boldsymbol{D}_{i}\right)
$$&lt;/p&gt;
&lt;p&gt;As we discussed, none of the factors in the first product involve any variable in $\boldsymbol{Y}$, and none in the second product involve any variable in $\boldsymbol{X}$. Hence, we can rewrite this product in the form:&lt;/p&gt;
&lt;p&gt;$$
P\left(X_{1}, \ldots, X_{n}\right)=\frac{1}{Z} f(\boldsymbol{X}, \boldsymbol{Z}) g(\boldsymbol{Y}, \boldsymbol{Z})
$$&lt;/p&gt;
&lt;p&gt;From this decomposition, the desired independence follows immediately (exercise 2.5). Now consider the case where $\boldsymbol{X} \cup \boldsymbol{Y} \cup \boldsymbol{Z} \subset \mathcal{X}$. Let $\boldsymbol{U}=\mathcal{X}-(\boldsymbol{X} \cup \boldsymbol{Y} \cup \boldsymbol{Z})$. We can
partition $\boldsymbol{U}$ into two disjoint sets $\boldsymbol{U}_{1}$ and $\boldsymbol{U}_{2}$ such that $\boldsymbol{Z}$ separates $\boldsymbol{X} \cup \boldsymbol{U}_{1}$ from $\boldsymbol{Y} \cup \boldsymbol{U}_{2}$ in $\mathcal{H}$. Using the preceding argument, we conclude that $P \models\left(\boldsymbol{X}, \boldsymbol{U}_{1} \perp \boldsymbol{Y}, \boldsymbol{U}_{2} \mid \boldsymbol{Z}\right)$. Using the
decomposition property $(\boldsymbol{X} \perp \boldsymbol{Y}, \boldsymbol{W} \mid \boldsymbol{Z}) \Longrightarrow(\boldsymbol{X} \perp \boldsymbol{Y} \mid \boldsymbol{Z})$ (equation 2.8), we conclude that $P \models(\boldsymbol{X} \perp \boldsymbol{Y} \mid \boldsymbol{Z})$.&lt;/p&gt;
&lt;p&gt;Let&amp;rsquo;s look at the theorem 4.1&lt;/p&gt;
&lt;p&gt;Theorem 4.1:&lt;/p&gt;
&lt;p&gt;Let $P$ be a distribution over $\mathcal{X}$, and $\mathcal{H}$ a Markov network structure over $\mathcal{X}$. If $P$ is a Gibbs distribution that factorizes over $\mathcal{H}$, then $\mathcal{H}$ is an I-map for $P$.&lt;/p&gt;
&lt;p&gt;The proof of theorem 4.1 is the same as the proof of soundness, so, the authors say that &amp;ldquo;We first consider the analogue to theorem 3.2, which asserts that a Gibbs distribution satisfies the independencies associated with the graph. In other words, this result states the soundness of the separation criterion.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;The other direction is the theorem 4.2.&lt;/p&gt;
&lt;p&gt;Theorem 4.2:&lt;/p&gt;
&lt;p&gt;Let $P$ be a positive distribution over $\mathcal{X}$, and $\mathcal{H}$ a Markov network graph over $\mathcal{X} .$ If $\mathcal{H}$ is an I-map for $P$, then $P$ is a Gibbs distribution that factorizes over $\mathcal{H}$.&lt;/p&gt;
&lt;p&gt;The author said &amp;ldquo;The theorem 4.1 and 4.2 shows the soundness of the seperation condition as a criterion for detecting independences in Markov network: any distribution that factorizes over (Definition 4.4) $$\mathcal{G} satisfies the independece assertions implied by seperation.&amp;rdquo;, however, I think only theorem 4.1 can proof the soundness.&lt;/p&gt;
&lt;p&gt;The completeness is to proof that.&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Koller, D., &amp;amp; Friedman, N. (2009). Probabilistic graphical models: principles and techniques. MIT press. &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Jordan, M. I. (2003). An introduction to probabilistic graphical models. &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
  </channel>
</rss>
