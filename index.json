[{"authors":null,"categories":null,"content":"I am a Ph.D. candidate at Santa Clara University supervised by Prof. Yi Fang.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am a Ph.D. candidate at Santa Clara University supervised by Prof. Yi Fang.","tags":null,"title":"Zhiyuan Peng","type":"authors"},{"authors":[],"categories":null,"content":"","date":1695034800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1695034800,"objectID":"ed734048430e2fbb3448e35c8aa99502","permalink":"https://zhiyuanpeng.github.io/talk/entity-aware-multi-task-learning-for-query-understanding-at-walmart/","publishdate":"2023-08-13T21:49:51-07:00","relpermalink":"/talk/entity-aware-multi-task-learning-for-query-understanding-at-walmart/","section":"event","summary":"","tags":[],"title":"Entity-aware Multi-task Learning for Query Understanding at Walmart","type":"event"},{"authors":["zpeng"],"categories":["PGM"],"content":"A demo to run inference algorithms of PGM.\n","date":1691987550,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1691987550,"objectID":"69445e8d74432d54a0521010d9395a2d","permalink":"https://zhiyuanpeng.github.io/project/interactive-pgm/","publishdate":"2023-08-13T21:32:30-07:00","relpermalink":"/project/interactive-pgm/","section":"project","summary":"A demo to run inference algorithms of PGM.","tags":["CS"],"title":"Interactive Probabilistic Graphical Model","type":"project"},{"authors":["zpeng","ywang","yfang"],"categories":["IR"],"content":"Semantic Segment Search (SSS) is a searching engine which empowers the users to search the semantic related results on segment level. This demo is developed for Palo Alto Research Center (PARC).\nSemantic Segment Search ","date":1691961383,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1691961383,"objectID":"9ad45c28473819ef6730372bc7560026","permalink":"https://zhiyuanpeng.github.io/project/semantic-segment-search/","publishdate":"2023-08-13T14:16:23-07:00","relpermalink":"/project/semantic-segment-search/","section":"project","summary":"Semantic Segment Search (SSS) is a searching engine which empowers the users to search the semantic related results on segment level. This demo is developed for Palo Alto Research Center (PARC).","tags":["CS"],"title":"Semantic Segment Search","type":"project"},{"authors":["Xiaoxiao Shang","Zhiyuan Peng","Qiming Yuan","Sabiq Khan","Lauren Xie","Yi Fang","Subramaniam Vincent"],"categories":["NLP"],"content":" DIANES ","date":1691960274,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1691960274,"objectID":"362770a411457ad04bbeae39677be1a2","permalink":"https://zhiyuanpeng.github.io/project/a-dei-audit-toolkit-for-news-sources/","publishdate":"2023-08-13T13:57:54-07:00","relpermalink":"/project/a-dei-audit-toolkit-for-news-sources/","section":"project","summary":" DIANES ","tags":["CS"],"title":"DIANES: A Dei Audit Toolkit for News Sources","type":"project"},{"authors":["Zhiyuan Peng","Behnoush Abdollahi","Min Xie","Yi Fang"],"categories":[],"content":"","date":1691957935,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1691957935,"objectID":"ce23f1ebb81fe297675db1a880ce74ed","permalink":"https://zhiyuanpeng.github.io/publication/peng-4295244-semi/","publishdate":"2023-08-13T20:18:55.447689Z","relpermalink":"/publication/peng-4295244-semi/","section":"publication","summary":"","tags":[],"title":"Semi-Supervised Named Entity Recognition with Data Augmentation by Structured Consistency Training","type":"publication"},{"authors":["Zhiyuan Peng","Vachik Dave","Nicole McNabb","Rahul Sharnagat","Alessandro Magnani","Ciya Liao","Yi Fang","Sravanthi Rajanala"],"categories":[],"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1691957936,"objectID":"6760c79ac2c6d7557efba0efd67f1236","permalink":"https://zhiyuanpeng.github.io/publication/peng-2023-entity/","publishdate":"2023-08-13T20:18:56.57235Z","relpermalink":"/publication/peng-2023-entity/","section":"publication","summary":"","tags":[],"title":"Entity-aware Multi-task Learning for Query Understanding at Walmart","type":"publication"},{"authors":["Zhiyuan Peng","Xuyang Wu","Yi Fang"],"categories":[],"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1691957935,"objectID":"31914929674f9870e045150bf43d55f9","permalink":"https://zhiyuanpeng.github.io/publication/peng-2023-soft/","publishdate":"2023-08-13T20:18:55.735743Z","relpermalink":"/publication/peng-2023-soft/","section":"publication","summary":"","tags":[],"title":"Soft Prompt Tuning for Augmenting Dense Retrieval with Large Language Models","type":"publication"},{"authors":["Xiaoxiao Shang","Zhiyuan Peng","Qiming Yuan","Sabiq Khan","Lauren Xie","Yi Fang","Subramaniam Vincent"],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1691957935,"objectID":"8083a05b9bda36303b12bae308ecb543","permalink":"https://zhiyuanpeng.github.io/publication/shang-2022-dianes/","publishdate":"2023-08-13T20:18:54.889379Z","relpermalink":"/publication/shang-2022-dianes/","section":"publication","summary":"","tags":[],"title":"DIANES: A DEI Audit Toolkit for News Sources","type":"publication"},{"authors":["zpeng"],"categories":[],"content":" Koller/Friedman\u0026rsquo;s book Probabilistic Graphical Models: Principles and Techniques 1 (PGM) is the most exhaustive book I have found for PGM, however, there are many definitions, theorems and mathematics within it which make the book hard to understand. When I read chapter 4 of this book, I meet two questions: one is why the potential functions are defined on cliques, another is why the d-separation works. For the first question, I didn\u0026rsquo;t find a good answer from Koller/Friedman\u0026rsquo;s book, but, I found Micheal I. Jordan\u0026rsquo;s unfinished book An Introduction to Probabilistic Graphical Model [^4] gives some intuitions of why we do it like that. For the second one, Koller/Friedman\u0026rsquo;s book defines the d-separation in definition 4.9 and proves the soundness in theorem 4.1. When I first read theorem 4.1, I didn\u0026rsquo;t understand why the proof of theorem 4.1 could prove the soundness of definition 4.9. I got it after I read that part several times and I will share what I learned in this post.\nPrerequisites Some Important Concepts If you feel Koller/Friedman\u0026rsquo;s PGM difficult to digest, you may not familiar with some concepts, so, we first clarify some concepts that I did not know when I first read this book:\ncompleteness: An axiom system is complete if all valid statements can be derived. soundness: An axiom system is sound if only valid statements can be derived. satisfiability: We say that a truth assignment $I$ satisfies a formula $\\varphi$ if $\\varphi[I]=\\mathrm{T} ;$ we write this as $I \\models \\varphi$. A formula $\\varphi$ is satisfiable if there exists a truth assignment $I$ such that $I \\models \\varphi$; otherwise $\\varphi$ is unsatisfiable. validity: A formula $\\varphi$ is valid (or a tautology) if for all a truth assignments $I, I \\models \\varphi$. Proof of Exercise 2.5 In Koller/Friedman\u0026rsquo;s PGM book, the conclusion of exercise 2.5 is directly used to prove other theorems, but no proof is given. I googled this answer on the Internet, luckily, I found it and the proof is right in my view.\nExercise 2.5:\nLet $X, Y, Z$ be three disjoint subsets of random variables. We say $X$ and $Y$ are conditionally independent given $Z$ if and only if\n$$ \\mathbb P_{X, Y \\mid Z}(x, y \\mid z)=\\mathbb P_{X \\mid Z}(x \\mid z) \\mathbb P_{Y \\mid Z}(y \\mid z) $$\nShow that $X$ and $Y$ are conditionally independent given $Z$ if and only if the joint distribution for the three subsets of random variables factors in the following form:\n$$ \\mathbb P_{X, Y, Z}(x, y, \\mid z)=h(x, z) g(y, z)) $$\nSolution:\nFist, we proof if $X$ and $Y$ are conditionally independent given $Z$, we can get $\\mathbb P_{X, Y, Z}(x, y, \\mid z)=h(x, z) g(y, z))$ as follows:\n$$ \\begin{aligned} \\mathbb P_{X, Y, Z}(x, y, z) \u0026amp;= \\mathbb P_{Z}(z) \\mathbb P_{X, Y | Z}(x, y | z) \\\\ \u0026amp;=\\mathbb P_{Z}(z) \\mathbb P_{X | Z}(x | z) \\mathbb P_{Y | Z}(y | z) \\\\ \u0026amp;=h(x, z) g(y, z) \\end{aligned} $$\nwhere we choose $h(x, z)=\\mathbb P_{X \\mid Z}(x \\mid z)$ and $g(y, z) = \\mathbb P_{Y \\mid Z}(y \\mid z) \\mathbb P_{Z}(z)$\nThen, we proof that if we can write $\\mathbb P_{X, Y, Z}(x, y, z)$ in this form: $h(x, z) g(y, z))$, we can get $\\mathbb P_{X, Y \\mid Z}(x, y \\mid z)=\\mathbb P_{X \\mid Z}(x \\mid z) \\mathbb P_{Y \\mid Z}(y \\mid z)$:\n$$ \\begin{aligned} \\mathbb P_{X, Y \\mid Z}(z, y \\mid z) \u0026amp;=\\frac{\\mathbb P_{X, Y, Z}(x, y, z)}{\\sum_{x, y} \\mathbb P_{X, Y, Z}(x, y, z)} \\\\ \u0026amp;=\\frac{h(x, z) g(y, z)}{\\sum_{x, y} h(x, z) g(y, z)} \\\\ \u0026amp;=\\frac{h(x, z)}{h_{1}(z)} \\frac{g(y, z)}{ g_{1}(z)} \\\\ \u0026amp;=\\mathbb P_{X \\mid Z}(x \\mid z) \\mathbb P_{Y \\mid Z}(y \\mid z) \\end{aligned} $$\nDefinition 4.4 We say that a distribution factorizes over a Markov network $\\mathcal{H}$ if each $\\boldsymbol{D}_{k}(k=1, \\ldots, K)$ is a complete subgraph of $\\mathcal{H}$.\nGibbs Distribution A distribution $P_{\\Phi}$ is a Gibbs distribution parameterized by a set of factors $\\Phi=\\left\\{\\phi_{1}\\left(\\boldsymbol{D}_{1}\\right), \\ldots, \\phi_{K}\\left(\\boldsymbol{D}_{K}\\right)\\right\\}$ if it is defined as follows: $$ P_{\\Phi}\\left(X_{1}, \\ldots, X_{n}\\right)=\\frac{1}{Z} \\tilde{P}{\\Phi}\\left(X{1}, \\ldots, X_{n}\\right) $$ where $$ \\tilde{P}{\\Phi}\\left(X{1}, \\ldots, X_{n}\\right)=\\phi_{1}\\left(\\boldsymbol{D}{1}\\right) \\times \\phi{2}\\left(\\boldsymbol{D}{2}\\right) \\times \\cdots \\times \\phi{m}\\left(\\boldsymbol{D}{m}\\right) $$ is an unnormalized measure and $$ Z=\\sum{X_{.}} \\tilde{P}{\\Phi}\\left(X{1}, \\ldots, X_{n}\\right) $$\nIf we say a Gibbs distribution which factorizes over a Markov network, each factor in this Gibbs distribution is over a clique or complete subgraph.\nDefinition 4.8 Let $\\mathcal{H}$ be a Markov network structure, and let $X_{1}-\\ldots-X_{k}$ be a path in $\\mathcal{H}$. Let $\\boldsymbol{Z} \\subseteq \\mathcal{X}$ be a set of observed variables. The path $X_{1}-\\ldots-X_{k}$ is active given $\\boldsymbol{Z}$ if none of the $X_{i}$ \u0026rsquo;s, $i=1, \\ldots, k$, is in $Z$.\nWhy UDG is defined on cliques? $A\u0026ndash;B\u0026ndash;C$\nFrom the graph above, we can get $A \\bot C \\mid B$, so, we don\u0026rsquo;t want $A$ and $C$ in the same potential function because they are independent if $B$ is given. We notice that there is no edge connecting $A$ and $C$ directly, so, we may have an intuition that nodes that are not connected to each other should not be included in the same potential function. This is the intuition why UDG is defined on cliques. We use potential functions to measure the relationship among nodes within the same clique, not the conditional probability or marginal probability. For instance, we can use the chain rule to write the distribution $P$ factorizing over the graph above like this: $P = P(A)P(B \\mid A)P(C \\mid B)$. If you push $P$ into two factors, you may select one of the two forms: $P = P(A,B)P(C \\mid B)$ and $P = P(B, C)P(A \\mid B)$. Don\u0026rsquo;t forget that our distribution defined on UDG can be factorized like this $P = \\Phi(A, B)\\Phi(B,C)$. Then, we can get the conclusion that not all the potential functions are marginal probabilities or conditional probabilities. In one word, potential functions are not probabilities. How can we make sure the distribution consisting of potential functions equals 1 if we summarize all the variables within the distribution? We need a normalizer $\\mathcal{Z}$. From definition 4.4 and the definition of Gibbs distribution, we know that we can use a Gibbs distribution which factorizes over the Markov network to represent the distribution expressed by the Markov network.\nKoller/Firedman says in the PGM book \u0026ldquo;if we define the UDG on cliques, we will not violate the independence assumptions induced by the network structure\u0026rdquo;. In my view, the authors here want to say that if we use the Gibbs distribution which factorizes over the Markov network to represent the distribution expressed by the network, we can prove the theorem 4.1 which also proves the soundness of the d-separation which is the independence assumptions induced by the network.\nWhy D-seperation works? Definition 4.9 defines the d-separation of UDG. The proof of definition 4.9 will answer this question.\nDefinition 4.9:\nWe say that a set of nodes $\\boldsymbol{Z}$ separates $\\boldsymbol{X}$ and $\\boldsymbol{Y}$ in $\\mathcal{H}$, denoted $\\operatorname{sep}_{\\mathcal{H}}(\\boldsymbol{X} ; \\boldsymbol{Y} \\mid \\boldsymbol{Z})$, if there is no active path between any node $X \\in X$ and $Y \\in \\boldsymbol{Y}$ given $\\boldsymbol{Z}$. We define the global independencies associated with $\\mathcal{H}$ to be:\n$$ \\mathcal{I}(\\mathcal{H})=\\left\\{(\\boldsymbol{X} \\perp \\boldsymbol{Y} \\mid \\boldsymbol{Z}): \\operatorname{sep}_{\\mathcal{H}}(\\boldsymbol{X} ; \\boldsymbol{Y} \\mid \\boldsymbol{Z})\\right\\} $$\nWe prove the soundness and completeness of definition 4.9. For definition 4.9, the soundness and completeness are defined as follows:\nIf we find that two nodes $X$ and $Y$ are d-separated given some $Z$, then we are guaranteed that they are, in fact, conditionally independent given $Z$. D-separation detects all possible independencies. More precisely, if we have that two variables $X$ and $Y$ are independent given $Z$, then they are d-separated. Before we proof the soundness, we know that there is a Gibbs distribution $\\mathcal{P}$ over $\\mathcal{X}$ factorizes over $\\mathcal{H}$ can be expressed by the Markov network $\\mathcal{H}$. Let $\\boldsymbol{X}, \\boldsymbol{Y}, \\boldsymbol{Z}$ be any three disjoint subsets in $\\mathcal{X}$ such that $\\boldsymbol{Z}$ separates $\\boldsymbol{X}$ and $\\boldsymbol{Y}$ in $\\mathcal{H}$. We want to show that $P \\models(\\boldsymbol{X} \\perp \\boldsymbol{Y} \\mid \\boldsymbol{Z})$.\nWe start by considering the case where $\\boldsymbol{X} \\cup \\boldsymbol{Y} \\cup \\boldsymbol{Z}=\\mathcal{X} .$ As $\\boldsymbol{Z}$ separates $\\boldsymbol{X}$ from $\\boldsymbol{Y}$, there are no direct edges between $\\boldsymbol{X}$ and $\\boldsymbol{Y}$. Hence, any clique in $\\mathcal{H}$ is fully contained either in $\\boldsymbol{X} \\cup \\boldsymbol{Z}$ or in $\\boldsymbol{Y} \\cup \\boldsymbol{Z}$. Let $\\mathcal{I}_{\\boldsymbol{X}}$ be the indexes of the set of cliques that are contained in $\\boldsymbol{X} \\cup \\boldsymbol{Z}$, and let $\\mathcal{I}_{Y}$ be the indexes of the remaining cliques. We know that\n$$ P\\left(X_{1}, \\ldots, X_{n}\\right)=\\frac{1}{Z} \\prod_{i \\in \\mathcal{I}{\\boldsymbol{X}}} \\phi{i}\\left(\\boldsymbol{D}{i}\\right) \\cdot \\prod{i \\in \\mathcal{I}{\\boldsymbol{Y}}} \\phi{i}\\left(\\boldsymbol{D}_{i}\\right) $$\nAs we discussed, none of the factors in the first product involve any variable in $\\boldsymbol{Y}$, and none in the second product involve any variable in $\\boldsymbol{X}$. Hence, we can rewrite this product in the form:\n$$ P\\left(X_{1}, \\ldots, X_{n}\\right)=\\frac{1}{Z} f(\\boldsymbol{X}, \\boldsymbol{Z}) g(\\boldsymbol{Y}, \\boldsymbol{Z}) $$\nFrom this decomposition, the desired independence follows immediately (exercise 2.5). Now consider the case where $\\boldsymbol{X} \\cup \\boldsymbol{Y} \\cup \\boldsymbol{Z} \\subset \\mathcal{X}$. Let $\\boldsymbol{U}=\\mathcal{X}-(\\boldsymbol{X} \\cup \\boldsymbol{Y} \\cup \\boldsymbol{Z})$. We can partition $\\boldsymbol{U}$ into two disjoint sets $\\boldsymbol{U}_{1}$ and $\\boldsymbol{U}_{2}$ such that $\\boldsymbol{Z}$ separates $\\boldsymbol{X} \\cup \\boldsymbol{U}_{1}$ from $\\boldsymbol{Y} \\cup \\boldsymbol{U}_{2}$ in $\\mathcal{H}$. Using the preceding argument, we conclude that $P \\models\\left(\\boldsymbol{X}, \\boldsymbol{U}_{1} \\perp \\boldsymbol{Y}, \\boldsymbol{U}_{2} \\mid \\boldsymbol{Z}\\right)$. Using the decomposition property $(\\boldsymbol{X} \\perp \\boldsymbol{Y}, \\boldsymbol{W} \\mid \\boldsymbol{Z}) \\Longrightarrow(\\boldsymbol{X} \\perp \\boldsymbol{Y} \\mid \\boldsymbol{Z})$ (equation 2.8), we conclude that $P \\models(\\boldsymbol{X} \\perp \\boldsymbol{Y} \\mid \\boldsymbol{Z})$.\nLet\u0026rsquo;s look at theorem 4.1\nTheorem 4.1:\nLet $P$ be a distribution over $\\mathcal{X}$, and $\\mathcal{H}$ a Markov network structure over $\\mathcal{X}$. If $P$ is a Gibbs distribution that factorizes over $\\mathcal{H}$, then $\\mathcal{H}$ is an I-map for $P$.\nThe proof of theorem 4.1 is the same as the proof of soundness, so, the authors say that \u0026ldquo;We first consider the analog to theorem 3.2, which asserts that a Gibbs distribution satisfies the independencies associated with the graph. In other words, this result states the soundness of the separation criterion.\u0026rdquo;\nI will talk about completeness in another post.\nKoller, D., \u0026amp; Friedman, N. (2009). Probabilistic graphical models: principles and techniques. MIT press.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","date":1620939570,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620939570,"objectID":"4a48031afb62b8c5e4970b13b2d79af1","permalink":"https://zhiyuanpeng.github.io/post/representation-of-undirected-graph-models/","publishdate":"2021-05-13T13:59:30-07:00","relpermalink":"/post/representation-of-undirected-graph-models/","section":"post","summary":"Koller/Friedman\u0026rsquo;s book Probabilistic Graphical Models: Principles and Techniques 1 (PGM) is the most exhaustive book I have found for PGM, however, there are many definitions, theorems and mathematics within it which make the book hard to understand.","tags":["pgm"],"title":"Representation of Undirected Graph Models","type":"post"},{"authors":["zpeng","bjiang","jluo"],"categories":[],"content":" PCB of the Weak Signal Detection Module ","date":1619325064,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1619325064,"objectID":"38cb7d9b7451d9b30c2ac27aa50858be","permalink":"https://zhiyuanpeng.github.io/project/electronic-position-detection-system/","publishdate":"2021-04-24T21:31:04-07:00","relpermalink":"/project/electronic-position-detection-system/","section":"project","summary":" PCB of the Weak Signal Detection Module ","tags":["EE"],"title":"Electronic Position Detection System","type":"project"},{"authors":["zpeng"],"categories":[],"content":" Head of Thermometer Tail of Thermometer Four Versions of the PCBs Communication with Monitor by Wireless Module ","date":1619325011,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1619325011,"objectID":"517ab66c171786e3e77aa03234ae8678","permalink":"https://zhiyuanpeng.github.io/project/non-contact-thermometer/","publishdate":"2021-04-24T21:30:11-07:00","relpermalink":"/project/non-contact-thermometer/","section":"project","summary":" Head of Thermometer Tail of Thermometer Four Versions of the PCBs Communication with Monitor by Wireless Module ","tags":["EE"],"title":"Non Contact Thermometer","type":"project"},{"authors":["zpeng","jluo","bjiang"],"categories":[],"content":" UI 1 UI 2 Car 1 Car 2 Car on the Race ","date":1619324966,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1619324966,"objectID":"e9e4d84241af215c2a24fac1acbd14c6","permalink":"https://zhiyuanpeng.github.io/project/intelligent-car-based-on-electromagnetic-sensors/","publishdate":"2021-04-24T21:29:26-07:00","relpermalink":"/project/intelligent-car-based-on-electromagnetic-sensors/","section":"project","summary":" UI 1 UI 2 Car 1 Car 2 Car on the Race ","tags":["EE"],"title":"Intelligent Car Based on Electromagnetic Sensors","type":"project"},{"authors":["fqin","jhao","zpeng"],"categories":[],"content":" Ready to Go! Install Laser Sensors ","date":1619319812,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1619319812,"objectID":"a000844746be9f7fa8f43b8cddd875bd","permalink":"https://zhiyuanpeng.github.io/project/intelligent-car-based-on-laser-sensors/","publishdate":"2021-04-24T20:03:32-07:00","relpermalink":"/project/intelligent-car-based-on-laser-sensors/","section":"project","summary":" Ready to Go! Install Laser Sensors ","tags":["EE"],"title":"Intelligent Car Based on Laser Sensors","type":"project"},{"authors":["Zhiyuan Peng","Behnoush Abdollahi","Min Xie","Yi Fang"],"categories":[],"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1691957934,"objectID":"f6ce5152c8ec7fcf908bd6a35d104a42","permalink":"https://zhiyuanpeng.github.io/publication/peng-2021-multi/","publishdate":"2023-08-13T20:18:53.626546Z","relpermalink":"/publication/peng-2021-multi/","section":"publication","summary":"","tags":[],"title":"Multi-label classification of short texts with label correlated recurrent neural networks","type":"publication"},{"authors":["Zhiyuan Peng"],"categories":[],"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1691957936,"objectID":"516bf389806c78affa0a83da2c10dcf2","permalink":"https://zhiyuanpeng.github.io/publication/peng-2020-classification/","publishdate":"2023-08-13T20:18:56.004865Z","relpermalink":"/publication/peng-2020-classification/","section":"publication","summary":"","tags":[],"title":"Classification and Named Entity Recognition of Short Texts","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne **Two** Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}} Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://zhiyuanpeng.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Shao-Zhong Lu","Sheng Li","Zhi-Yuan Peng","Zhen-Gang Tang"],"categories":[],"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1691957935,"objectID":"f727f0bfc099a628385ab8cbed018c1b","permalink":"https://zhiyuanpeng.github.io/publication/lu-2016-high/","publishdate":"2023-08-13T20:18:55.163955Z","relpermalink":"/publication/lu-2016-high/","section":"publication","summary":"","tags":[],"title":"High-precision TT\u0026C signal simulation technology based on Lagrange interpolation in high dynamic environment","type":"publication"},{"authors":["Yuyao Shen","Yongqing Wang","Zhiyuan Peng","Siliang Wu"],"categories":[],"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1691957936,"objectID":"da8fd01494d968b1020046171e178063","permalink":"https://zhiyuanpeng.github.io/publication/shen-2016-multiple/","publishdate":"2023-08-13T20:18:56.298154Z","relpermalink":"/publication/shen-2016-multiple/","section":"publication","summary":"","tags":[],"title":"Multiple-access interference mitigation for acquisition of code-division multiple-access continuous-wave signals","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f26b5133c34eec1aa0a09390a36c2ade","permalink":"https://zhiyuanpeng.github.io/admin/config.yml","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/admin/config.yml","section":"","summary":"","tags":null,"title":"","type":"wowchemycms"}]